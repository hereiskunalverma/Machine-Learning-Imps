{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP6OqltguVWz"
   },
   "source": [
    "`On for a powerful face recognition and verification prefer to your course on Deep Learning`\n",
    "\n",
    "`For OpenCV implementation of powerful face recognition, prefer to https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-shDFUJMmBbP"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We are working on **Face Recognition** with OpenCV (Open Source Computer Vision).\n",
    "\n",
    "To create a complete project on Face Recognition, we must work on 3 very distinct phases:\n",
    "\n",
    "  - Face Detection and Data Gathering\n",
    "  - Train the Recognizer\n",
    "  - Face Recognition\n",
    "\n",
    "The below block diagram resumes those phases:\n",
    "\n",
    "![picture](https://miro.medium.com/max/1020/0*oJIRaoERCUHoyylG.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USBuq-uZo3B5"
   },
   "source": [
    "## Testing camera\n",
    "\n",
    "This will output the results both in RGB and GRAY formats\n",
    "\n",
    "`Note` --> Run it on locally machine with WebCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KOlu47TxlsEF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640) # set Width\n",
    "cap.set(4,480) # set Height\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    #frame = cv2.flip(frame, -1) # Flip camera vertically\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('gray', gray)\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27: # press 'ESC' to quit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNIjoH8VqFYJ"
   },
   "source": [
    "## Face Detection\n",
    "\n",
    "![picture](https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/face-recognition-opencv/face_recognition_opencv_animation_01.gif)\n",
    "\n",
    "How does it work ?\n",
    "\n",
    "**The secret is a technique called deep metric learning.**\n",
    "\n",
    "If you have any prior experience with deep learning you know that we typically train a network to:\n",
    "\n",
    "  - Accept a single input image\n",
    "  - And output a classification/label for that image\n",
    "\n",
    "However, deep metric learning is different.\n",
    "\n",
    "Instead, of trying to output a single label (or even the coordinates/bounding box of objects in an image), we are instead outputting a real-valued feature vector.\n",
    "\n",
    "For the dlib facial recognition network, the output feature vector is 128-d (i.e., a list of 128 real-valued numbers) that is used to quantify the face. Training the network is done using triplets:\n",
    "\n",
    "![picture](https://www.pyimagesearch.com/wp-content/uploads/2018/06/face_recognition_opencv_triplet.jpg)\n",
    "\n",
    "Here we provide three images to the network:\n",
    "\n",
    "  - Two of these images are example faces of the same person.\n",
    "  - The third image is a random face from our dataset and is not the same person as the other two images.\n",
    "\n",
    "As an example, let’s again consider Figure above where we provided three images: one of Chad Smith and two of Will Ferrell.\n",
    "\n",
    "Our network quantifies the faces, constructing the 128-d embedding (quantification) for each.\n",
    "\n",
    "From there, the general idea is that we’ll tweak the weights of our neural network so that the 128-d measurements of the two Will Ferrel will be closer to each other and farther from the measurements for Chad Smith.\n",
    "\n",
    "Our network architecture for face recognition is based on ResNet-34 from the [Deep Residual Learning for Image Recognition paper by He et al.](https://arxiv.org/abs/1512.03385), but with fewer layers and the number of filters reduced by half.\n",
    "\n",
    "The network itself was trained by [Davis King](https://pyimagesearch.com/2017/03/13/an-interview-with-davis-king-creator-of-the-dlib-toolkit/) on a dataset of ~3 million images. On the [Labeled Faces in the Wild (LFW) dataset](http://vis-www.cs.umass.edu/lfw/) the network compares to other state-of-the-art methods, reaching `99.38% accuracy`.\n",
    "\n",
    "Both Davis King (the creator of dlib) and Adam Geitgey (the author of the face_recognition module we’ll be using shortly) have written detailed articles on how deep learning-based facial recognition works:\n",
    "\n",
    "  - [High Quality Face Recognition with Deep Metric Learning ](http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html) (Davis)\n",
    "\n",
    "  - [Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78) (Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsEPrd29r8lK"
   },
   "source": [
    "## Let Create the Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGx5MG81uvtk"
   },
   "source": [
    "## Some little talk\n",
    "\n",
    "The most basic task on Face Recognition is of course, “Face Detecting”. Before anything, you must “capture” a face (Phase 1) in order to recognize it, when compared with a new face captured on future (Phase 3).\n",
    "\n",
    "The most common way to detect a face (or any objects), is using the [“Haar Cascade classifier”](https://docs.opencv.org/3.3.0/d7/d8b/tutorial_py_face_detection.html)\n",
    "\n",
    "Object Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, “Rapid Object Detection using a Boosted Cascade of Simple Features” in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.\n",
    "\n",
    "Here we will work with face detection. Initially, the algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier. Then we need to extract features from it. The good news is that OpenCV comes with a trainer as well as a detector. If you want to train your own classifier for any object like car, planes etc. you can use OpenCV to create one. Its full details are given here: [Cascade Classifier Training.](https://docs.opencv.org/3.3.0/dc/d88/tutorial_traincascade.html)\n",
    "\n",
    "\n",
    "If you do not want to create your own classifier, OpenCV already contains many pre-trained classifiers for face, eyes, smile, etc. Those XML files can be download from [haarcascades](https://github.com/Itseez/opencv/tree/master/data/haarcascades) directory.\n",
    "\n",
    "Enough theory, let’s create a face detector with OpenCV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF-yuTK0r71-",
    "outputId": "1450a38a-6524-485f-a628-199a5d07d33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: face_recognition in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
      "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.18.0)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (0.3.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.18.5)\n",
      "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.1.2)\n",
      "Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "# installing some libraries\n",
    "!pip install face_recognition\n",
    "!pip install imutils # A series of convenience functions to make basic image processing functions such as \n",
    "                     # translation, rotation, resizing, skeletonization, \n",
    "                     # and displaying Matplotlib images easier with OpenCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ta5X9e_yo_2Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') # cascade classifier front face only\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640) # set Width\n",
    "cap.set(4, 480) # set Height\n",
    "\n",
    "while True:\n",
    "\tret, img = cap.read()\n",
    "\t#img = cv2.flip(img, -1) # Flip camera vertically\n",
    "\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\tfaces = faceCascade.detectMultiScale(\n",
    "\t\tgray, # input gray scale\n",
    "\t\tscaleFactor=1.2, # Specifying how much the image size is reduced at each image scale.\n",
    "                     # Used to create the scale pyramid.\n",
    "\t\tminNeighbors=5,  # Specifying how many neighbors each candidate rectangle should have, to retain it.\n",
    "                     # A higher number gives lower positives\n",
    "\t\tminSize=(20, 20)\n",
    "\t\t)\n",
    " \n",
    " # The function below will detect faces on the image. \n",
    " # Next, we must “mark” the faces in the image, using, for example, a blue rectangle. \n",
    " # This is done with this portion of the code:\n",
    "\n",
    "\tfor (x, y, w, h) in faces:\n",
    "\t\tcv2.rectangle(img, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\t\troi_gray = gray[y:y+h, x:x+w]\n",
    "\t\troi_color = img[y:y+h, x:x+w]\n",
    " # If faces are found, it returns the positions of detected faces as a rectangle with the left up corner (x,y) \n",
    " # and having “w” as its Width and “h” as its Height ==> (x,y,w,h).\n",
    "\n",
    "\tcv2.imshow('video', img)\n",
    "\n",
    "\tk = cv2.waitKey(30)\n",
    "\tif k == 27: # press 'ESC' to quit\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEWLGclp1YhT"
   },
   "source": [
    "# Now putting labels with the faces classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mOdD8f11fgj"
   },
   "source": [
    "## Data Gathering\n",
    "\n",
    "Let’s start the first phase of our project. What we will do here, is starting from last step (Face Detecting), we will simply create a dataset, where we will store for each id, a group of photos in gray with the portion that was used for face detecting.\n",
    "![picture](https://miro.medium.com/max/960/0*Nuf1sgV1y5DaH6wF.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hO4TfUo91c40"
   },
   "outputs": [],
   "source": [
    "# create directory to store our facial samples\n",
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "enter user id and press ==> 2\n",
      "\n",
      " [INFO] Initializing face capture. Look the camera and wait ... \n",
      "\n",
      " [INFO] Exiting Program and cleanup stuff\n"
     ]
    }
   ],
   "source": [
    "# This code will create dataset for training\n",
    "import cv2\n",
    "import os\n",
    "import cv2\n",
    "import os\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(3, 640) # set video width\n",
    "cam.set(4, 480) # set video height\n",
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# For each person, enter one numeric face id\n",
    "face_id = input('\\nenter user id and press ==> ')\n",
    "print('\\n [INFO] Initializing face capture. Look the camera and wait ... ')\n",
    "# Initialize individual sampling face count\n",
    "count = 0\n",
    "while(True):\n",
    "    ret, img = cam.read()\n",
    "    #img = cv2.flip(img, -1) # flip video image vertically\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)     \n",
    "        count += 1\n",
    "        # Save the captured image into the datasets folder\n",
    "        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' +  \n",
    "                    str(count) + \".jpg\", gray[y:y+h,x:x+w])\n",
    "        cv2.imshow('image', img)\n",
    "    k = cv2.waitKey(100) & 0xff # Press 'ESC' for exiting video\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif count >= 30: # Take 30 face sample and stop video\n",
    "         break\n",
    "# Do a bit of cleanup\n",
    "print(\"\\n [INFO] Exiting Program and cleanup stuff\")\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the dataset created above \n",
    "\n",
    "![picture](https://miro.medium.com/max/1020/0*N4IcbE8v2nwgj6Xg.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating subdirectory to store the trained data\n",
    "!mkdir trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INFO] Training faces. It will take a few seconds. Wait ...\n",
      "\n",
      " [INFO] 1 faces trained. Exiting Program\n"
     ]
    }
   ],
   "source": [
    "# This will train the dataset created above for training\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "# Path for face image database\n",
    "path = 'dataset'\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create() # the LBPH (LOCAL BINARY PATTERNS HISTOGRAMS) Face Recognizer\n",
    "detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\");\n",
    "# function to get the images and label data\n",
    "# returns 2 array: \"ids\" and \"faces\" to train the recognizer\n",
    "def getImagesAndLabels(path):\n",
    "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]     \n",
    "    faceSamples=[]\n",
    "    ids = []\n",
    "    for imagePath in imagePaths:\n",
    "        PIL_img = Image.open(imagePath).convert('L') # grayscale\n",
    "        img_numpy = np.array(PIL_img,'uint8')\n",
    "        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n",
    "        faces = detector.detectMultiScale(img_numpy)\n",
    "        for (x,y,w,h) in faces:\n",
    "            faceSamples.append(img_numpy[y:y+h,x:x+w])\n",
    "            ids.append(id)\n",
    "    return faceSamples,ids\n",
    "print (\"\\n [INFO] Training faces. It will take a few seconds. Wait ...\")\n",
    "faces,ids = getImagesAndLabels(path)\n",
    "recognizer.train(faces, np.array(ids))\n",
    "# Save the model into trainer/trainer.yml\n",
    "recognizer.write('trainer/trainer.yml') \n",
    "# Print the numer of faces trained and end program\n",
    "print(\"\\n [INFO] {0} faces trained. Exiting Program\".format(len(np.unique(ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we reached the final phase of our project. Here, we will capture a fresh face on our camera and if this person had his face captured and trained before, our recognizer will make a “prediction” returning its id and an index, shown how confident the recognizer is with this match.\n",
    "\n",
    "![picture](https://miro.medium.com/max/947/0*kkZMQyWtR5NOFr3q.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INFO] Exiting Program and cleanup stuff\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read('trainer/trainer.yml')\n",
    "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#iniciate id counter\n",
    "id = 0\n",
    "# names related to ids: example ==> Marcelo: id=1,  etc\n",
    "names = ['None', 'Kunal Verma', 'Paula', 'Ilza', 'Z', 'W'] \n",
    "# Initialize and start realtime video capture\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(3, 640) # set video widht\n",
    "cam.set(4, 480) # set video height\n",
    "# Define min window size to be recognized as a face\n",
    "minW = 0.1*cam.get(3)\n",
    "minH = 0.1*cam.get(4)\n",
    "while True:\n",
    "    ret, img =cam.read()\n",
    "    #img = cv2.flip(img, -1) # Flip vertically\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale( \n",
    "        gray,\n",
    "        scaleFactor = 1.2,\n",
    "        minNeighbors = 5,\n",
    "        minSize = (int(minW), int(minH)),\n",
    "       )\n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "        id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
    "        \n",
    "        # If confidence is less then 100 ==> \"0\" : perfect match \n",
    "        if (confidence < 100):\n",
    "            id = names[id]\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        else:\n",
    "            id = \"unknown\"\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        \n",
    "        cv2.putText(\n",
    "                    img, \n",
    "                    str(id), \n",
    "                    (x+5,y-5), \n",
    "                    font, \n",
    "                    1, \n",
    "                    (255,255,255), \n",
    "                    2\n",
    "                   )\n",
    "        cv2.putText(\n",
    "                    img, \n",
    "                    str(confidence), \n",
    "                    (x+5,y+h-5), \n",
    "                    font, \n",
    "                    1, \n",
    "                    (255,255,0), \n",
    "                    1\n",
    "                   )  \n",
    "    \n",
    "    cv2.imshow('camera',img) \n",
    "    k = cv2.waitKey(10) & 0xff # Press 'ESC' for exiting video\n",
    "    if k == 27:\n",
    "        break\n",
    "# Do a bit of cleanup\n",
    "print(\"\\n [INFO] Exiting Program and cleanup stuff\")\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Face Detection with labels (OpenCV).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
